{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from nano_model import TransformerConfig, TransformerLMHead\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "context = nullcontext() if device == \"mps\" else torch.autocast(device)\n",
    "pin_memory = device == \"cuda\"\n",
    "pin_memory_device = device if device == \"cuda\" else \"\"\n",
    "\n",
    "N_EMBD = 384\n",
    "N_LAYER = 6\n",
    "N_HEAD = 6\n",
    "\n",
    "BLOCK_SIZE = 128\n",
    "BATCH_SIZE = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str | list[str], char2int: dict[str, int]) -> Tensor:\n",
    "    if isinstance(text, list):\n",
    "        return torch.tensor([[char2int[c] for c in s if c in char2int] for s in text])\n",
    "    else:\n",
    "        return torch.tensor([char2int[c] for c in text if c in char2int])\n",
    "\n",
    "\n",
    "def decode(y: list[int] | Tensor, int2char: dict[int, str]) -> str:\n",
    "    return \"\".join([int2char[int(i)] for i in y if int(i) in int2char])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_plain.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "chars = sorted(list(set(test_text)))\n",
    "vocab_size = len(chars)\n",
    "assert vocab_size == 14\n",
    "\n",
    "char2int = {c: i for i, c in enumerate(chars)}\n",
    "int2char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TransformerConfig(\n",
    "    n_positions=BLOCK_SIZE,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=N_LAYER,\n",
    "    n_head=N_HEAD,\n",
    "    n_embd=N_EMBD,\n",
    ")\n",
    "\n",
    "model = TransformerLMHead(config).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"models/plain_decoder.pt\", weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$12+234=246$\n"
     ]
    }
   ],
   "source": [
    "prompt = \"$12+234=\"\n",
    "encoded_prompt = encode(prompt, char2int)[None].to(device)\n",
    "output = model.generate(encoded_prompt, max_new_tokens=4)\n",
    "decoded_output = decode(output[0], int2char)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = test_text.split(\"\\n\")[:-1]\n",
    "line_lens = [len(line) for line in lines]\n",
    "line_eq_idxs = [line.find(\"=\") for line in lines]\n",
    "lines_data = [t for t in zip(line_lens, line_eq_idxs)]\n",
    "\n",
    "line_groups = defaultdict(list)\n",
    "\n",
    "for line, line_data in zip(lines, lines_data):\n",
    "    line_groups[line_data].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "batch_eq_idxs = []\n",
    "\n",
    "for (_, eq_idx), grouped_lines in line_groups.items():\n",
    "    for i in range(0, len(grouped_lines), BATCH_SIZE):\n",
    "        batch = grouped_lines[i : i + BATCH_SIZE]\n",
    "        batches.append(encode(batch, char2int))\n",
    "        batch_eq_idxs.append(eq_idx)\n",
    "        assert torch.all(batches[-1][:, batch_eq_idxs[-1]] == char2int[\"=\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[accuracy: 1.00]: 100%|██████████| 48/48 [01:30<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "n_total = 0\n",
    "\n",
    "progress_bar = tqdm(zip(batches, batch_eq_idxs), desc=\"accuracy: \", total=len(batches))\n",
    "\n",
    "for batch, eq_idx in progress_bar:\n",
    "    batch = batch.to(device)\n",
    "    input_ids = batch[:, :eq_idx + 1]\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=5)\n",
    "    target = batch[:, eq_idx + 1:]\n",
    "    output_ids = output_ids[:, eq_idx + 1 : batch.shape[1]]\n",
    "    n_correct += torch.sum(torch.all(output_ids == target, dim=1).int()).item()\n",
    "    n_total += len(output_ids)\n",
    "    progress_bar.set_description(f\"[accuracy: {n_correct / n_total:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
